{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 5b: Creating a Decision RNN (80 pts)\n",
    "\n",
    "In this assignment, you will perform a simplified variant of the RNN analyses in Mante*, Sussillo* et al., Nature, 2013 (MS2013). You will train an RNN to perform evidence accumulation (a.k.a. integration), and visualize various features of your trained RNN. Specifically, you will train an RNN to output the running sum of a time-series stimulus input, mimicking the evidence accumulation computation that is central to many simple decision-making tasks. You will visualize the output of the trained RNN on one trial for each stimulus strength, and you will display the condition-averaged activity of a few of the RNN's hidden units. You will then visualize low-dimensional representations of the high-dimensional hidden states of the RNN using PCA and a regression technique akin to Targeted Dimensionality Reduction from MS2013. These visualization techniques are a first step toward understanding how the RNN is representing information and performing computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Create the Synthetic Data (5 pts)\n",
    "First, create the data (inputs and target outputs) used to train the RNN. \n",
    "\n",
    "For the inputs, you will generate synthetic time-series stimuli, which represents the time-varying random dots stimuli from MS2013. The stimulus will be two-dimensional, one representing motion energy and the other representing the average color. For each of these two dimensions, you will generate time-series stimuli from univariate Gaussian distributions whose mean represents the corresponding stimulus strength. In a neuroscience experiment, these stimulus strengths might be referred to as \"conditions\" and would allow the experimenter to control the difficulty of each trial.\n",
    "\n",
    "Draw 40 trials of 25-timestep stimuli for each condition (200 trials total) from unit-variance Gaussians with condition-specific means of {-1, -0.5, 0, 0.5, 1}. Each time step should be independent. You can use the `torch.randn` function to generate samples from a standard Gaussian and then shift by the corresponding mean. When finished, your stimuli should be a single Tensor with shape (200, 25, 2), where the first dimension is over trials, the second dimension is over timesteps, and the third dimension is over stimulus features (motion and color).\n",
    "\n",
    "Next, you will generate a context cue, which indicates to the RNN whether the motion or the color signal is relevant for its decision in a given trial. For each trial, randomly assign the context cue to be either +1 or -1 (each with probability 0.5), indicating that the relevant stimulus is motion or color, respectively. To indicate a constant context cue within each trial, replicate the cue along the time dimension of a Tensor of shape (200, 25, 1). Concatenate this with the stimulus, so that you obtain a Tensor of shape (200, 25, 3). This will be the input to your RNN.\n",
    "\n",
    "Now, create the target outputs for the RNN. For each trial, create a running sum across the time dimension of the relevant stimulus (i.e., the motion dimension or the color dimension, depending on the context cue for that trial). The running sum represents the time-varying integrated evidence for each decision. You can compute the running sum with the `torch.cumsum` function. When finished, your target output should be a Tensor with shape (200, 25, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Implement an RNN class (5 pts)\n",
    "Implement a class for constructing and running an RNN with a linear readout layer. Your class should inherit from the base class `nn.Module`. Please implement the `__init__` and `forward` methods. The `__init__` method should construct the RNN and readout layer, while the `forward` method should run the RNN and then pass its hidden states through the readout. Make sure that you return both the readout and the RNN hidden states, as we will be analyzing the latter after training the network. Note that you can directly use the RNN class presented in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Implement a Dataset class (5 pts)\n",
    "Implement a dataset class which inherits from the `Dataset` base class. This module should implement the `__init__`, `__len__`, and `__getitem__` methods. The `__init__` method should store the inputs and outputs (which should already be Tensors) passed via arguments to the method, the `__len__` method should return the size of the dataset, and `__getitem__` should return the datapoint at the given index passed via an argument. Note that you can use the same dataset class from the last homework assignment here, except do not wrap the inputs and outputs in a `torch.tensor`, as they are already Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Implement the Main Training Loop (10 pts)\n",
    "Implement a function which iterates through a dataset and trains the model over a specified number of epochs. In addition to what we showed in class, please add gradient clipping by norm using the `nn.utils.clip_grad_norm_` fuction and make the norm value threshold an argument to the training loop function.\n",
    "\n",
    "As in the previous assignment, create a Python list and append the current average loss at each epoch. You can compute the average loss over all the batches for a given epoch by keeping a running sum of the loss and dividing by the # of batches, which you can get with `len(dataloader)`. Be sure to zero the running sum after each epoch. Also, make sure to append only the value of the average loss to the list using `avg_loss.item()`, which returns the value of the scalar tensor as a standard Python number. \n",
    "\n",
    "We also want to keep track of the average magnitude of the gradients during training, which can be an important debugging tool. To get the average gradient magnitude of your network per batch, use the following function after calling the `backward()` method on your loss:\n",
    "\n",
    "`np.mean([p.grad.norm() for p in model.parameters()])`\n",
    "\n",
    "Just like with the loss, we want to keep a running average of this quantity each epoch as well and append it to its own separate list. Have the function return the loss and grad_norm lists after training. Additionally, display the epoch, average loss, and average grad_norm to the console every so often, where the display frequency (or period) is an argument to the function. This can be done with a simple `print` statement.\n",
    "\n",
    "Note that you can use basically the same training loop function from Programming Assignment 2, except you have to be careful to pass the RNN outputs to the loss function, not the hidden states or the network output tuple. Additionally, add gradient clipping by norm and make the norm value threshold an argument to the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Construct the Model, Dataset, and Other Bits (5 pts)\n",
    "First, create a specific RNN using our class. Then create an optimizer (use Adam this time) and pass the parameters of the model as an argument. Additionally, instantiate a loss function for our regression task (MSE loss). Finally, create a dataset using our dataset class and the data you created above. Wrap this dataset in a `DataLoader`, which will handle batching of the data. Please create an RNN which takes in 3 input, has a hidden layer of 32 neurons, has 1 output, and uses a Tanh activation function. Use a learning rate of 1e-4 and a batch size of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Train the Model (0 pts)\n",
    "Train your model using your training loop for 15000 epochs and clipping the gradient norm to a value of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Generate Average Loss Plot (10 pts)\n",
    "Plot the average loss and gradient norms during training. Create two subplots with \"Epoch\" on the x-axis and  \"Average Loss\" and \"Average Gradient Norm\" on the y-axes, respectively. For the average loss, use a log scale for the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 8: Visualize Network Outputs (10 pts)\n",
    "For one trial per stimulus-strength condition, plot the single-trial RNN readout, overlaid with the target output (1 panel per condition --> 5 panels total). To do this, you will have to run the RNN on the stimuli you select and display the RNN's readout. Note that you have to detach the output from the computational graph to plot it. This can either be done with the `detach()` function or by using the `torch.no_grad()` context. Please use a legend on the plot to indicate which is the RNN readout and which is the target output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9: Visualize Hidden States (10 pts)\n",
    "Now plot the condition-averaged activity from 3 individual hidden units (pick any unit that you like). To do this, first run the RNN $^1$ on all trials (if you haven't already stored the result above). Filter for the trials when the context cue is $+1$. Then, for each condition, compute the mean and standard deviation of the hidden states across all trials from that condition. This should involve operating on the first dimension in the Tensor containing the hidden unit activity (i.e., do not average across time steps or hidden units). For each of the hidden units you choose: create a new figure and plot the condition-averaged means and standard deviations $^2$ of the unit's activity. This should result in 5 25-timestep traces overlaid on the figure (one for each condition). Include: a title to indicate the unit #; appropriate axis labels; and a legend to indicate the conditions, labeled by the means of each stimulus distribution. \n",
    "\n",
    "Can you find 3 units with distinct representations?\n",
    "\n",
    "$^1$ Note that you will again have to detach the output from the computational graph (as you will for all proceeding tasks).\n",
    "\n",
    "$^2$ You can plot the standard deviation using the `fill_between` function in matplotlib.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 10: Visualize PCA Dimensions of Hidden States (10 pts)\n",
    "\n",
    "Now run the RNN on the entire dataset and compute the first two principle components of the hidden states. We can compute the principle components using an SVD with `torch.svd`. Note that you will have to reshape the hidden states tensor to be (-1, n_hidden), placing both the batch and sequence time step dimensions together. The SVD function should return three Tensors, which we will call `U`, `S`, and `Vh`. Take the `Vh` tensor, extract the mapping to the first two principle component directions, and then multiply this matrix with the hidden states to get our first two principle components.\n",
    "\n",
    "For each context ($+1$ or $-1$), filter the trials and plot a separate plot. Similar to above, compute condition-average PCA trajectories: for each condition, average across all trials in that condition, preserving the time (length=25) and features (length=2) dimensions. Plot the condition-averaged PC trajectories on a 2D plot, where the X axis is the first PCA dimension, and the Y axis is the second PCA dimension. For clarity, also include a marker ('o'), which indicates the first timestep of each trajectory. As in the previous section, include a legend to indicate the conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 11: Identifying the dimension in the hidden states that best relates to stimulus strength (10 pts)\n",
    "Finally, we will use ridge regression to identify the dimension in the space of the RNN hidden states that best predicts stimulus strength. This is similar to the Targeted Dimensionality Reduction (TDR) approach used in MS2013. \n",
    "\n",
    "---------------\n",
    "**Ridge Regression.**\n",
    "To create your regression inputs, take the hidden states of the network for the whole dataset and stack them in a (num_trials*num_timesteps, num_units) matrix. To create your regression targets, use the stimulus strengths (means of condition-specific Gaussians) that correspond to each (trial, time) in the leading dimension of the regression inputs. You can use any method you want to compute ridge regression model, but it is straightforward to implement it manually. If you implement it manually, make sure to appropriately append ones to the regression inputs to allow an offset term. The optimal parameters estimate of a ridge regression model is:\n",
    "\n",
    "$W = (X^T X + \\lambda I)^{-1} X^T Y$\n",
    "\n",
    "Note that this is equivalent to linear regression with L2 regularization on the parameters. Use an L2 scaling of $\\lambda=0.1$. Once you fit the model, extract the weights and the bias. Now apply the linear model to the hidden states, and plot the condition-averaged results versus time. \n",
    "\n",
    "---------------\n",
    "\n",
    "You should produce a figure in the style of Part 9, but here the y-axis indicates activity along the stimulus-strength dimension. Again, filter for trials where context is $+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 12: Visualize Targeted Dimensions of Hidden States (20 pts)\n",
    "\n",
    "Now we will attempt to recreate Fig. 5 in MS2013. We will do so by doing a simplified version of targeted dimensionality reduction (no orthogonalization). The steps are as follows:\n",
    "\n",
    "1. Run the RNN on the entire dataset.\n",
    "\n",
    "2. Perform ridge regression that predicts the mean values (conditions) from the hidden states for **motion context trials only**. The one-dimensional $W$ matrix found by ridge regression is the \"motion axis\". Project all data onto this motion axis (\"motion projection\"). See part 11 for details on how to do this.\n",
    "\n",
    "3. Do the same with **color context trials only** to find the \"color axis\" and \"color projection\".\n",
    "\n",
    "4. Perform ridge regression that predicts the choice (i.e. your target output values) from the hidden states for **all** trials. This gives the \"choice axis\". Again, project all data onto this axis (\"choice projection\").\n",
    "\n",
    "5. Plot the following four plots on a 2x2 figure (in order, from top to bottom, left to right):\n",
    "\n",
    "    (i) Plot motion context trials onto the motion-choice plane (x-axis is choice axis projection, y-axis is motion projection). \n",
    "\n",
    "        For motion context trials only, for each condition, average across the motion and choice projections. This should give you a (25,) array for each of the 5 conditions. Plot each of the arrays onto the motion-choice plane. If you want to make it look very clear and similar to Fig 5 in the paper, consider giving each line a different color or marker.\n",
    "\n",
    "    (ii) Plot color context trials onto the motion-choice plane.\n",
    "\n",
    "        Same as 5-(i), but changes context.\n",
    "\n",
    "    (iii) Plot motion context trials onto the color-choice plane.\n",
    "\n",
    "    (iv) Plot color context trials onto the color-choice plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Question (2 pts)\n",
    "\n",
    "What are a few differences between the training procedure in the paper vs the assignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Administrative (2 pts)\n",
    "\n",
    "About how many hours did you spend on this homework? There is no right or wrong answer :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "cse599n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
