{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Questions 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for plotting, do not need to change\n",
    "\n",
    "def set_spines_invisible(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "def set_grid_invisible(ax):\n",
    "    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    ax.xaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "    ax.yaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "    ax.zaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "\n",
    "def common_label(fig, xlabel, ylabel):\n",
    "    \"\"\"Put a common `xlabel` and `ylabel` on the figure `fig`.\n",
    "    \n",
    "    Args:\n",
    "        - fig (plt.figure)\n",
    "        - xlabel (str)\n",
    "        - ylabel (str)\n",
    "    \"\"\"\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "X_real = np.load(\"data/hw3-q1_train.npy\")\n",
    "num_trials = 91\n",
    "num_class = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(a)\n",
    "\n",
    "Plot the square-root of the eigenvalues of the data covariance matrix, sorted in decreasing order. The values indicate the standard deviation of the high-dimensional data along each of the principal component directions.\n",
    "\n",
    "Optional: Note that the square root eigenvalues are the singular values of a simple transformation of the data matrix. If you like, you may use SVD instead of eigenvalue decomposition for this question, but make sure you are taking the SVD of the correct matrix! Hint: write out the sample covariance as a function of the original data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- YOUR CODE HERE ----- #\n",
    "sqrt_eigvals = NotImplemented # singular values\n",
    "# -------------------------- #\n",
    "\n",
    "PC_numbers = np.arange(1,num_neurons+1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.plot(PC_numbers, sqrt_eigvals, c=\"k\", marker=\"o\", mfc=\"r\", mec=\"k\")\n",
    "ax.set_ylabel(r\"$\\sqrt{\\lambda_i}$\")  \n",
    "ax.set_xlabel(\"Principal component number, i\")\n",
    "set_spines_invisible(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- YOUR CODE HERE ----- #\n",
    "r = NotImplemented # number of dominant singular values\n",
    "explained_var = NotImplemented # explained variance of top r singular values\n",
    "# -------------------------- #\n",
    "\n",
    "print(f\"There appears to be an elbow after the {r}th dominant singular value. The top {r} singular values explain {explained_var*100:.2f}% of the data variance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# ----- YOUR CODE HERE ----- #\n",
    "# \n",
    "# -------------------------- #\n",
    "\n",
    "ax.set_xlabel(\"PCA 1\")\n",
    "ax.set_ylabel(\"PCA 2\")\n",
    "ax.set_zlabel(\"PCA 3\")\n",
    "set_grid_invisible(ax)\n",
    "\n",
    "# ----- YOUR CODE HERE ----- #\n",
    "# Find the rotation angle that best separates the clusters\n",
    "# \n",
    "# -------------------------- #\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "# ----- YOUR CODE HERE ----- #\n",
    "# Use \"coolwarm\" as your cmap\n",
    "# \n",
    "# -------------------------- #\n",
    "\n",
    "ax.set_xlabel(\"neuron #\")\n",
    "ax.set_ylabel(\"principal component #\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- YOUR CODE HERE ----- #\n",
    "ans = NotImplemented\n",
    "# -------------------------- #\n",
    "\n",
    "print(f\"Are there groupings? \\nA: {ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "X_sim = np.load(\"data/hw3-q2_sim.npy\")\n",
    "N, p = X_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "# ----- YOUR CODE HERE ----- #\n",
    "# \n",
    "# -------------------------- #\n",
    "\n",
    "ax.set_title(\"PCA projection\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "set_spines_invisible(ax)\n",
    "plt.ylim(5, 22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(b)\n",
    "\n",
    "Notations used in the code below:\n",
    "$\\begin{equation}\n",
    "x_n = Wz_n + \\mu + \\epsilon_n,\\ \\ \\epsilon_n \\sim \\mathcal{N}(0, \\Psi)\n",
    "\\end{equation}$\n",
    "where the variable for $\\mu$ is named ``mu``, $\\Psi$ is denoted as ``Psi``, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- YOUR CODE HERE ----- #\n",
    "def em_algorithm(X: np.array, zDim: int, typ: str = \"FA\", tol: float = 1e-8, cyc: int = int(1e8), minVarFrac: float = 0.01, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Performs EM algorithm for FA or PPCA.\n",
    "\n",
    "    Args:\n",
    "        X: data matrix, shape = (xDim, num_trials)\n",
    "        zDim: dimension of latent factors\n",
    "        typ: type of analysis, \"FA\" or \"PPCA\"\n",
    "        tol: stopping criterion for EM algorithm\n",
    "        cyc: maximum number of EM iterations\n",
    "        minVarFrac: fraction of overall data variance for each observed dimension\n",
    "            to set as the private variance floor.  This is used to combat \n",
    "            Heywood cases, where ML parameter learning returns one or more \n",
    "            zero private variances.\n",
    "            (See Martin & McDonald, Psychometrika, Dec 1975.)\n",
    "        verbose: whether to display status message\n",
    "    Returns:\n",
    "        estParams (dict): estimated parameters, keys: \"W\", \"Psi\", \"mu\"\n",
    "            W: Loadings matrix, shape = (xDim, zDim)\n",
    "            Psi: Diagonal of the observation noise covariance, shape = (xDim)\n",
    "                Note: For PPCA, all elements of Psi should be the same (\\sigma^2).\n",
    "                      For FA, each element may be different (Psi[i] = \\sigma^2_i).\n",
    "            mu: The empirical data mean, shape = (xDim)\n",
    "        LL (array-like): log-likelihood values for each EM iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Initialization of parameters\n",
    "    xDim, N = X.shape\n",
    "    cX = np.cov(X, ddof=0)\n",
    "    scale = np.mean(np.diag(cX))\n",
    "    W = np.random.randn(xDim, zDim) * np.sqrt(scale/zDim)\n",
    "    Psi = np.diag(cX)\n",
    "    mu = np.mean(X, axis=1)\n",
    "    varFloor = minVarFrac * np.diag(cX)\n",
    "    \n",
    "    # Set up log-likelihood\n",
    "    LLi = 0\n",
    "    LL = []\n",
    "    \n",
    "    # EM iterations loop\n",
    "    for i in range(1, cyc+1):\n",
    "        \n",
    "        # Compute data log likelihood\n",
    "        LLold = LLi\n",
    "        # **********************************************************************\n",
    "        # YOUR CODE HERE\n",
    "        # LLi = ?? # Log likelihood of the data under the iteration i parameters\n",
    "        raise NotImplementedError\n",
    "        # **********************************************************************\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Iteration {i}: LL {LLi}\")\n",
    "        LL.append(LLi)\n",
    "        \n",
    "        # E-step\n",
    "        # **********************************************************************\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "        # **********************************************************************\n",
    "        \n",
    "        # M-step\n",
    "        # **********************************************************************\n",
    "        # YOUR CODE HERE\n",
    "        # W = ??\n",
    "        # Psi = ??\n",
    "        raise NotImplementedError\n",
    "        # **********************************************************************\n",
    "\n",
    "        if typ == \"PPCA\":\n",
    "            ss = np.maximum(varFloor, np.mean(Psi))\n",
    "            Psi = ss * np.ones(xDim)\n",
    "        elif typ == \"FA\":\n",
    "            Psi = np.maximum(varFloor, Psi)\n",
    "        else:\n",
    "            raise ValueError(f\"``typ`` {typ} is not supported.\")\n",
    "\n",
    "        # Termination criterion\n",
    "        if i <= 2: # Guarantees that we do at least two EM iterations\n",
    "            LLbase = LLi\n",
    "        elif LLi < LLold: # Determine whether there are ever likelihood violations\n",
    "            print('VIOLATION')\n",
    "        elif (LLi - LLbase) < (1 + tol) * (LLold - LLbase): # Terminate EM\n",
    "            break\n",
    "\n",
    "    if np.any(Psi == varFloor):\n",
    "        print('Warning: Private variance floor used for one or more observed dimensions in FA.')\n",
    "    \n",
    "    estParams = {'W': W, 'Psi': Psi, 'mu': mu}\n",
    "    return estParams, LL\n",
    "# -------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- YOUR CODE HERE ----- # \n",
    "ppca_est, ppca_ll = NotImplemented\n",
    "# -------------------------- #\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.plot(ppca_ll, \"b\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"LL\")\n",
    "ax.set_title(\"PPCA Log-likelihood\")\n",
    "set_spines_invisible(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- YOUR CODE HERE ----- # \n",
    "\n",
    "# The sample constrained covariance estimated by PPCA (i.e., the modeled cov[x])\n",
    "C = NotImplemented\n",
    "\n",
    "# The empirical data covariance\n",
    "S = NotImplemented\n",
    "# -------------------------- #\n",
    "\n",
    "print(\"The observation noise variances estimated by PPCA are: \")\n",
    "print(Psi)\n",
    "print(\"\\nThe sample covariance estimated by PPCA is: \")\n",
    "print(C)\n",
    "print(\"\\nThis should be very close to the empirical data covariance: \")\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "# ----- YOUR CODE HERE ----- #\n",
    "#\n",
    "# -------------------------- #\n",
    "\n",
    "ax.set_title(\"PPCA projection\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "set_spines_invisible(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- YOUR CODE HERE ----- # \n",
    "fa_est, fa_ll = NotImplemented\n",
    "# -------------------------- #\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.plot(fa_ll, \"b\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"LL\")\n",
    "ax.set_title(\"FA Log-likelihood\")\n",
    "set_spines_invisible(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- YOUR CODE HERE ----- # \n",
    "\n",
    "# The sample constrained covariance estimated by PPCA (i.e., the modeled cov[x])\n",
    "C = NotImplemented\n",
    "\n",
    "# The empirical data covariance\n",
    "S = NotImplemented\n",
    "# -------------------------- #\n",
    "\n",
    "print(\"The observation noise variances estimated by FA are: \")\n",
    "print(Psi)\n",
    "print(\"\\nThe sample covariance estimated by FA is: \")\n",
    "print(C)\n",
    "print(\"\\nThis should be very close to the empirical data covariance: \")\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "# ----- YOUR CODE HERE ----- #\n",
    "# \n",
    "# -------------------------- #\n",
    "\n",
    "ax.set_title(\"FA projection\")\n",
    "set_spines_invisible(ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
